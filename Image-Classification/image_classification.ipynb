{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = datasets.MNIST('data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is a 28x28 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=28x28 at 0x7FF2D55837C0>, 5)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAB4AHgBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiijrTvLf+435Vf0vRrrVpSkCHI9q3rf4d6xcsQidPasmPw1fSaudNVD5wOOldnafBLxLdwiVFXafasPXvh1q/h5WN3t+XrxXHkYOKKKKKciGRgq9TXb6J8Mda1uyFzbxEqfauv0b4N6grD7ZAce4rtLL4QWCzRmW3G3vxXXr8LfDAQKbIEgdauWHw/0DTnLW9qFJqTU7bSdCtTPIioDxzXAafP4Yj8QnUJAmSc5rrbr4k6DaR7IpV4HArw/4j+OU1m4dIpAVPpXk5OSTSUUUU+OQxuGHUV9Q/CTxTZL4cjtppFEnFekf2/p5nWETjzG6CtLPGe1cx4n8X2Hh4AXEoQn3rG0b4n6LdzGOW6UZ6c1z/xo8QR/8ItHJZS7t3cGvnX+373GN5/OoJNUuZTlnP51Ud2kbcxyabRRRRRWzpHiK60h1MLHA966PTfHmo3HiC2kZjjcB1r6h0PVp9S8PtcEfvFTivmv4rarqF7qWy6DKqtx7153BcSW8qyI5BBzwa29T8WX2qaallOcxr05rn6KKKKKKKKKntLg2l1HOoyUOa+kPhN47i1C0FpdOqMflwTVP41eEXu44rq0iLd/lFfP8umXkDESW7jHtVUgg4IwaSiiiiiiiiiitTQtVm0rU4J45GVVYEgGvqrwh4u0/wAXaYkF0YwyKB8x61Jqvw4sNQSRognzA4wK+ePGXgO50W+mZI2KbjjiuGdGjYqwwR2ptFFFFFFFFFFa2j6/e6Rco8M7qoOSAa948HfGOB44rW5+ZuASTXp2paRZeJ9IWVYkzIuQcV86/Eb4dTaEZb5R+7znAry2iiiiiiiiiip7a0lupAkaEk+1dhpHw91m6lilhyvzA19YeF7WWy8O2lvMcyImDXH/ABdvLOHwxIJlBPNfJ91Ikk7NGMKagoooooooqSGF55BHGpJNd14Y+GuqatMkhjPl554r3Xwz8L9OsY0a6t1349K7u20OwtFAihAxUOq+ILDQ4S1w4CgdjXg/xU+IGl69p8tnasC9eH0UUUUUVYsrKfULgQW6FnPQV12n/DLXrpgTbNg+1exeBvhLawW6yanBiUeor1fTtGtNKj22yBQPaoNT8S6dpKk3UwXHvXlfi/4vW8Ksum3A3exrxbW/H2s6yzrPMShPAzXKsxdizHJNJRRRRQOSBXovgj4ft4hmTzIiUavbfDnwf0vRrhbnaPMHavQEgtNOi3FVUAdcVDJr+mxRl3uVCjqa4Hxh8UtPsYHTT7pXcDsa+fvEHj3U9clcSudmTjmuTZi7FmJJPrSUUUUUUUA4INd94Z+KOoeGkVbeFW2jFdOf2g9b/wCfZf0rN1H43azqMZR49oPoa5q68faldWzwsWAbvmuUeWSRizOxJ9TTKKKKKKKKKKKKKKKKKKKKKKK//9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=120x120>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds_train[0])\n",
    "ds_train[0][0].resize((120,120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform the data to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "ds_train = datasets.MNIST('data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get only the images \n",
    "ims_train = ds_train.data\n",
    "ims_train = ims_train.float() / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu = torch.mean(ims_train)\n",
    "std = torch.std(ims_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the data as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mu, std)\n",
    "])\n",
    "ds_train = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "ds_test = datasets.MNIST('data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.4241), tensor(2.8215))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0][0].min(), ds_train[0][0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to receive mini-batches, not only single data points.\n",
    "We use PyTorch's DataLoader class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "#########################################################################\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, num_workers= NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in Pytorch\n",
    "\n",
    "Let's build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nInput = 784\n",
    "nOutput = 10\n",
    "nLayer = 2\n",
    "nHidden = 16\n",
    "act_fn = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, nInput, nOutput, nLayer, nHidden, act_fn):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [] \n",
    "        \n",
    "        layers.append(nn.Linear(nInput,nHidden))\n",
    "        layers.append(act_fn)\n",
    "        for i in range(nLayer-2):\n",
    "            layers.append(nn.Linear(nHidden,nHidden))\n",
    "            layers.append(act_fn)\n",
    "\n",
    "        layers.append(nn.Linear(nHidden,nOutput))\n",
    "        \n",
    "        ###############################\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if the forward pass works\n",
    "# should print torch.Size([1, 10])\n",
    "t = torch.randn(1,1,28,28)\n",
    "mlp = MLP(nInput, nOutput, nLayer, nHidden, act_fn)\n",
    "mlp(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dl_test, device='cpu'):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dl_test:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(dl_test.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        test_loss, correct, len(dl_test.dataset),\n",
    "        100. * correct / len(dl_test.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, dl_train, optimizer, epoch, log_interval=100, device='cpu'):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    train_loss=0\n",
    "    for batch_idx, (data, target) in enumerate(dl_train):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # zero the gradient, otherwise PyTorch would accumulate them\n",
    "        optimizer.zero_grad()         \n",
    "        \n",
    "       \n",
    "        output= model(data)\n",
    "        loss= F.cross_entropy(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+= loss.item()\n",
    "        ###############################\n",
    "\n",
    "        # stats\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(dl_train.dataset),\n",
    "                100. * batch_idx / len(dl_train), loss.item()))\n",
    "\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        loss, correct, len(dl_train.dataset),\n",
    "        100. * correct / len(dl_train.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use Adam ad an Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlp = MLP(nInput, nOutput, nLayer, nHidden, act_fn)\n",
    "optimizer = optim.Adam(mlp.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.331825\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.406160\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.385242\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.309892\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.285654\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.187300\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.481597\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.447424\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.533116\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.249155\n",
      "\n",
      "Train set: Average loss: 0.2093, Accuracy: 53041/60000 (88.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2590, Accuracy: 9240/10000 (92.400%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.195265\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.151395\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.349043\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.240895\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.349235\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.240095\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.323441\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.272171\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.390288\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.113808\n",
      "\n",
      "Train set: Average loss: 0.1226, Accuracy: 55716/60000 (92.9%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2213, Accuracy: 9367/10000 (93.670%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.187709\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.229654\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.992678\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.167740\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.222404\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.166637\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.131705\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.205981\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.188417\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.259453\n",
      "\n",
      "Train set: Average loss: 0.1259, Accuracy: 56315/60000 (93.9%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2046, Accuracy: 9421/10000 (94.210%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.112598\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.189164\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.195685\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.267143\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.113448\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.241035\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.098661\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.328983\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.069395\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.219505\n",
      "\n",
      "Train set: Average loss: 0.3042, Accuracy: 56600/60000 (94.3%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1906, Accuracy: 9454/10000 (94.540%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.080385\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.081405\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.292864\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.112219\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.080965\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.216455\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.216326\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.038618\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.223878\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.271509\n",
      "\n",
      "Train set: Average loss: 0.0766, Accuracy: 56854/60000 (94.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1826, Accuracy: 9468/10000 (94.680%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.176911\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.203975\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.082005\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.077080\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.190827\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.104958\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.286991\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.137419\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.091669\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.218260\n",
      "\n",
      "Train set: Average loss: 0.1853, Accuracy: 57013/60000 (95.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1740, Accuracy: 9489/10000 (94.890%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.254004\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.206370\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.148156\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.181756\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.109029\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.073095\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.119436\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.165001\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.280068\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.095444\n",
      "\n",
      "Train set: Average loss: 0.0304, Accuracy: 57127/60000 (95.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1772, Accuracy: 9489/10000 (94.890%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.170873\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.191367\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.133794\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.134700\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.093213\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.056120\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.116523\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.081327\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.230568\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.075432\n",
      "\n",
      "Train set: Average loss: 0.0733, Accuracy: 57255/60000 (95.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1706, Accuracy: 9507/10000 (95.070%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.064116\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.042755\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.218013\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.142712\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.113619\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.230767\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.187527\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.044712\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.222324\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.202009\n",
      "\n",
      "Train set: Average loss: 0.1600, Accuracy: 57369/60000 (95.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1707, Accuracy: 9522/10000 (95.220%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.243206\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.125577\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.238303\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.111196\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.078557\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.122868\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.088601\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.153229\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.088887\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.089493\n",
      "\n",
      "Train set: Average loss: 0.6741, Accuracy: 57496/60000 (95.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1608, Accuracy: 9521/10000 (95.210%)\n",
      "\n",
      "Training is finished.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(mlp, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(mlp, dl_test)\n",
    "\n",
    "print ('Training is finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some parameter tuning to boost the test accuracy to > **97%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308430\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.591933\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.176761\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.538406\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.112436\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.495725\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.095287\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.262857\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.174011\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.167180\n",
      "\n",
      "Train set: Average loss: 0.1447, Accuracy: 53623/60000 (89.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1734, Accuracy: 9448/10000 (94.480%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.088201\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.057698\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.119408\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.241391\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.129327\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.207890\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.046584\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.307582\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.082253\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.076160\n",
      "\n",
      "Train set: Average loss: 0.0432, Accuracy: 57448/60000 (95.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1324, Accuracy: 9570/10000 (95.700%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.073083\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.206323\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.184054\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.041319\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.255331\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.092223\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.196695\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.156965\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.069260\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.162435\n",
      "\n",
      "Train set: Average loss: 0.0151, Accuracy: 58093/60000 (96.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1148, Accuracy: 9645/10000 (96.450%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.022958\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.082342\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.142630\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.069983\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.161288\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.071558\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.063360\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.237957\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.103346\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.032690\n",
      "\n",
      "Train set: Average loss: 0.2158, Accuracy: 58407/60000 (97.3%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1012, Accuracy: 9692/10000 (96.920%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.024340\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.113458\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.183857\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.041938\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.060738\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.052533\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.057510\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.083068\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.063702\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.021797\n",
      "\n",
      "Train set: Average loss: 0.0689, Accuracy: 58581/60000 (97.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1025, Accuracy: 9697/10000 (96.970%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.051736\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.027317\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.037404\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.009843\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.046995\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.021765\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.012168\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.004659\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.053920\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.038309\n",
      "\n",
      "Train set: Average loss: 0.1667, Accuracy: 58804/60000 (98.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0928, Accuracy: 9727/10000 (97.270%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.035499\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.037493\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.049274\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.032563\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.040717\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.077881\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.093992\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.017854\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.086908\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.061805\n",
      "\n",
      "Train set: Average loss: 0.1273, Accuracy: 58969/60000 (98.3%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0860, Accuracy: 9747/10000 (97.470%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.005680\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.004977\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.056081\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.050616\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.017465\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006280\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.011453\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.112270\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.028670\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.016724\n",
      "\n",
      "Train set: Average loss: 0.0214, Accuracy: 59057/60000 (98.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0896, Accuracy: 9733/10000 (97.330%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.022133\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.036187\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.022464\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.003608\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.013058\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.153419\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.045291\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.033325\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.021601\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.006177\n",
      "\n",
      "Train set: Average loss: 0.0207, Accuracy: 59141/60000 (98.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0905, Accuracy: 9742/10000 (97.420%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.050764\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.015674\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.037299\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.038953\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.045108\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.045615\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.106156\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.025229\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.010134\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.078676\n",
      "\n",
      "Train set: Average loss: 0.0013, Accuracy: 59224/60000 (98.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9748/10000 (97.480%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.005549\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.004567\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.016508\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.009015\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.027204\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.066091\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.004851\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.007655\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.005249\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.016551\n",
      "\n",
      "Train set: Average loss: 0.0504, Accuracy: 59289/60000 (98.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9727/10000 (97.270%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.042330\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.009594\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.030074\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.065128\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.016854\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.023919\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.003042\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.046796\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.051024\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.132573\n",
      "\n",
      "Train set: Average loss: 0.1237, Accuracy: 59344/60000 (98.9%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0966, Accuracy: 9756/10000 (97.560%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.040230\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.002556\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.001712\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.002504\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.006800\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.005721\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.039589\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.044103\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.022611\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.002476\n",
      "\n",
      "Train set: Average loss: 0.0006, Accuracy: 59377/60000 (99.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0907, Accuracy: 9769/10000 (97.690%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.074097\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.007826\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.024392\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.012106\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.004476\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.037017\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.053387\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.003873\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.012648\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.055986\n",
      "\n",
      "Train set: Average loss: 0.0050, Accuracy: 59482/60000 (99.1%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0989, Accuracy: 9765/10000 (97.650%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.052756\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.061266\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.045103\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.034273\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.007151\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.001296\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.059815\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.001483\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.014252\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.029034\n",
      "\n",
      "Train set: Average loss: 0.0065, Accuracy: 59415/60000 (99.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1047, Accuracy: 9731/10000 (97.310%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.003689\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.002005\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.015988\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.001526\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.045379\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.123618\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.028945\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.004896\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.001564\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.029575\n",
      "\n",
      "Train set: Average loss: 0.0350, Accuracy: 59531/60000 (99.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0923, Accuracy: 9772/10000 (97.720%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.003666\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.000993\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.021842\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.009024\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.004020\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.016943\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.003096\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.007288\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.004405\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.001786\n",
      "\n",
      "Train set: Average loss: 0.0992, Accuracy: 59522/60000 (99.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1072, Accuracy: 9748/10000 (97.480%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.002551\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.035364\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.119015\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.015162\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.000357\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.005204\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.006436\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.012233\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.004797\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.026621\n",
      "\n",
      "Train set: Average loss: 0.0704, Accuracy: 59510/60000 (99.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1120, Accuracy: 9733/10000 (97.330%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.007370\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.004076\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.000834\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.002134\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.002157\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.006522\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.002490\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.000598\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.000241\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.056368\n",
      "\n",
      "Train set: Average loss: 0.0300, Accuracy: 59567/60000 (99.3%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1181, Accuracy: 9754/10000 (97.540%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.020568\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 0.003933\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.000451\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.058056\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.012697\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.000488\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.014753\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 0.000317\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.032127\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.015182\n",
      "\n",
      "Train set: Average loss: 0.0016, Accuracy: 59621/60000 (99.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1032, Accuracy: 9779/10000 (97.790%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.003282\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 0.025836\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.007309\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 0.013904\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.012426\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 0.000080\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.000442\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 0.000232\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.000560\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 0.004509\n",
      "\n",
      "Train set: Average loss: 0.0239, Accuracy: 59627/60000 (99.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1074, Accuracy: 9774/10000 (97.740%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.010743\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 0.027818\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.008516\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 0.087065\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.010551\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 0.032948\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.014935\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 0.009319\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.066753\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 0.131320\n",
      "\n",
      "Train set: Average loss: 0.0008, Accuracy: 59589/60000 (99.3%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1143, Accuracy: 9749/10000 (97.490%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.004062\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 0.000295\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.003131\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 0.000801\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.000748\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 0.002212\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.000242\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 0.001516\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.004286\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 0.000919\n",
      "\n",
      "Train set: Average loss: 0.0179, Accuracy: 59646/60000 (99.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1030, Accuracy: 9765/10000 (97.650%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.006473\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 0.013436\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.000771\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 0.005945\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.000577\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 0.000274\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.001708\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 0.001594\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.001008\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 0.033431\n",
      "\n",
      "Train set: Average loss: 0.0117, Accuracy: 59681/60000 (99.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1242, Accuracy: 9766/10000 (97.660%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.007134\n",
      "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 0.012166\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.014841\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 0.001002\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.001131\n",
      "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 0.002128\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.000706\n",
      "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 0.016210\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.000522\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 0.033767\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 59683/60000 (99.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1121, Accuracy: 9773/10000 (97.730%)\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.002436\n",
      "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 0.000603\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 0.001093\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 0.000123\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.025130\n",
      "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 0.001474\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.008142\n",
      "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 0.020909\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.005523\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 0.000341\n",
      "\n",
      "Train set: Average loss: 0.0341, Accuracy: 59677/60000 (99.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1195, Accuracy: 9736/10000 (97.360%)\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.001695\n",
      "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 0.004230\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 0.001214\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 0.003627\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.029477\n",
      "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 0.000473\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.045256\n",
      "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 0.028346\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.002421\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 0.003629\n",
      "\n",
      "Train set: Average loss: 0.0008, Accuracy: 59684/60000 (99.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1192, Accuracy: 9767/10000 (97.670%)\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.010883\n",
      "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 0.000163\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 0.009601\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 0.009471\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.004046\n",
      "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 0.015617\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.000124\n",
      "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 0.005254\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.002915\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 0.026985\n",
      "\n",
      "Train set: Average loss: 0.0101, Accuracy: 59746/60000 (99.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1513, Accuracy: 9700/10000 (97.000%)\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.000859\n",
      "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 0.000192\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 0.002203\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 0.002185\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.003912\n",
      "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 0.002280\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.007174\n",
      "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 0.116524\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.000217\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 0.000455\n",
      "\n",
      "Train set: Average loss: 0.0272, Accuracy: 59682/60000 (99.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1196, Accuracy: 9742/10000 (97.420%)\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.001618\n",
      "Train Epoch: 30 [6400/60000 (11%)]\tLoss: 0.001625\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.021600\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 0.000694\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.000037\n",
      "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 0.015090\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.028954\n",
      "Train Epoch: 30 [44800/60000 (75%)]\tLoss: 0.052778\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.022453\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 0.006264\n",
      "\n",
      "Train set: Average loss: 0.0064, Accuracy: 59758/60000 (99.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1264, Accuracy: 9763/10000 (97.630%)\n",
      "\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.002258\n",
      "Train Epoch: 31 [6400/60000 (11%)]\tLoss: 0.000324\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 0.001640\n",
      "Train Epoch: 31 [19200/60000 (32%)]\tLoss: 0.000140\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.000299\n",
      "Train Epoch: 31 [32000/60000 (53%)]\tLoss: 0.000542\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.027340\n",
      "Train Epoch: 31 [44800/60000 (75%)]\tLoss: 0.001689\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.035196\n",
      "Train Epoch: 31 [57600/60000 (96%)]\tLoss: 0.011122\n",
      "\n",
      "Train set: Average loss: 0.1731, Accuracy: 59672/60000 (99.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1269, Accuracy: 9754/10000 (97.540%)\n",
      "\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.018092\n",
      "Train Epoch: 32 [6400/60000 (11%)]\tLoss: 0.024920\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 0.095232\n",
      "Train Epoch: 32 [19200/60000 (32%)]\tLoss: 0.000663\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.112641\n",
      "Train Epoch: 32 [32000/60000 (53%)]\tLoss: 0.011950\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.015662\n",
      "Train Epoch: 32 [44800/60000 (75%)]\tLoss: 0.017072\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.046764\n",
      "Train Epoch: 32 [57600/60000 (96%)]\tLoss: 0.024164\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 59715/60000 (99.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1065, Accuracy: 9781/10000 (97.810%)\n",
      "\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.030364\n",
      "Train Epoch: 33 [6400/60000 (11%)]\tLoss: 0.000888\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 0.000250\n",
      "Train Epoch: 33 [19200/60000 (32%)]\tLoss: 0.000023\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.006036\n",
      "Train Epoch: 33 [32000/60000 (53%)]\tLoss: 0.000149\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.065935\n",
      "Train Epoch: 33 [44800/60000 (75%)]\tLoss: 0.000245\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.009253\n",
      "Train Epoch: 33 [57600/60000 (96%)]\tLoss: 0.029494\n",
      "\n",
      "Train set: Average loss: 0.1749, Accuracy: 59813/60000 (99.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1441, Accuracy: 9749/10000 (97.490%)\n",
      "\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.013060\n",
      "Train Epoch: 34 [6400/60000 (11%)]\tLoss: 0.000496\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 0.004605\n",
      "Train Epoch: 34 [19200/60000 (32%)]\tLoss: 0.012788\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.000550\n",
      "Train Epoch: 34 [32000/60000 (53%)]\tLoss: 0.006452\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.006150\n",
      "Train Epoch: 34 [44800/60000 (75%)]\tLoss: 0.000714\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.002263\n",
      "Train Epoch: 34 [57600/60000 (96%)]\tLoss: 0.000029\n",
      "\n",
      "Train set: Average loss: 0.0003, Accuracy: 59731/60000 (99.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1449, Accuracy: 9741/10000 (97.410%)\n",
      "\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.005346\n",
      "Train Epoch: 35 [6400/60000 (11%)]\tLoss: 0.061735\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.007828\n",
      "Train Epoch: 35 [19200/60000 (32%)]\tLoss: 0.000330\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.005324\n",
      "Train Epoch: 35 [32000/60000 (53%)]\tLoss: 0.019104\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.001488\n",
      "Train Epoch: 35 [44800/60000 (75%)]\tLoss: 0.004839\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.000318\n",
      "Train Epoch: 35 [57600/60000 (96%)]\tLoss: 0.058666\n",
      "\n",
      "Train set: Average loss: 0.0001, Accuracy: 59748/60000 (99.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1409, Accuracy: 9734/10000 (97.340%)\n",
      "\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.035556\n",
      "Train Epoch: 36 [6400/60000 (11%)]\tLoss: 0.036488\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 0.000324\n",
      "Train Epoch: 36 [19200/60000 (32%)]\tLoss: 0.000732\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.002390\n",
      "Train Epoch: 36 [32000/60000 (53%)]\tLoss: 0.000105\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 0.025837\n",
      "Train Epoch: 36 [44800/60000 (75%)]\tLoss: 0.022300\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.000316\n",
      "Train Epoch: 36 [57600/60000 (96%)]\tLoss: 0.056142\n",
      "\n",
      "Train set: Average loss: 0.0011, Accuracy: 59806/60000 (99.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1254, Accuracy: 9761/10000 (97.610%)\n",
      "\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.000194\n",
      "Train Epoch: 37 [6400/60000 (11%)]\tLoss: 0.052898\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 0.001200\n",
      "Train Epoch: 37 [19200/60000 (32%)]\tLoss: 0.008244\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.000183\n",
      "Train Epoch: 37 [32000/60000 (53%)]\tLoss: 0.110079\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 0.000725\n",
      "Train Epoch: 37 [44800/60000 (75%)]\tLoss: 0.011014\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.003104\n",
      "Train Epoch: 37 [57600/60000 (96%)]\tLoss: 0.001187\n",
      "\n",
      "Train set: Average loss: 0.2451, Accuracy: 59761/60000 (99.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1201, Accuracy: 9771/10000 (97.710%)\n",
      "\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.000153\n",
      "Train Epoch: 38 [6400/60000 (11%)]\tLoss: 0.001904\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 0.003054\n",
      "Train Epoch: 38 [19200/60000 (32%)]\tLoss: 0.032901\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.001383\n",
      "Train Epoch: 38 [32000/60000 (53%)]\tLoss: 0.004311\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 0.007522\n",
      "Train Epoch: 38 [44800/60000 (75%)]\tLoss: 0.000282\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.000796\n",
      "Train Epoch: 38 [57600/60000 (96%)]\tLoss: 0.003117\n",
      "\n",
      "Train set: Average loss: 0.0062, Accuracy: 59745/60000 (99.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1246, Accuracy: 9768/10000 (97.680%)\n",
      "\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.000233\n",
      "Train Epoch: 39 [6400/60000 (11%)]\tLoss: 0.000406\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 0.004414\n",
      "Train Epoch: 39 [19200/60000 (32%)]\tLoss: 0.000802\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.023136\n",
      "Train Epoch: 39 [32000/60000 (53%)]\tLoss: 0.000751\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 0.000114\n",
      "Train Epoch: 39 [44800/60000 (75%)]\tLoss: 0.033415\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.000474\n",
      "Train Epoch: 39 [57600/60000 (96%)]\tLoss: 0.001278\n",
      "\n",
      "Train set: Average loss: 0.0002, Accuracy: 59833/60000 (99.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1477, Accuracy: 9781/10000 (97.810%)\n",
      "\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.000161\n",
      "Train Epoch: 40 [6400/60000 (11%)]\tLoss: 0.000026\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 0.000014\n",
      "Train Epoch: 40 [19200/60000 (32%)]\tLoss: 0.031565\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.000166\n",
      "Train Epoch: 40 [32000/60000 (53%)]\tLoss: 0.001109\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.003866\n",
      "Train Epoch: 40 [44800/60000 (75%)]\tLoss: 0.000490\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.004362\n",
      "Train Epoch: 40 [57600/60000 (96%)]\tLoss: 0.004180\n",
      "\n",
      "Train set: Average loss: 0.0000, Accuracy: 59781/60000 (99.6%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1316, Accuracy: 9781/10000 (97.810%)\n",
      "\n",
      "Training is finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nLayer = 5\n",
    "nHidden = 64\n",
    "act_fn = nn.ReLU()\n",
    "\n",
    "# reinitialize the mlp, so we can play with parameters right here\n",
    "mlp = MLP(nInput, nOutput, nLayer, nHidden, act_fn)\n",
    "optimizer = optim.Adam(mlp.parameters())\n",
    "\n",
    "epochs = 40\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(mlp, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(mlp, dl_test)\n",
    "\n",
    "print ('Training is finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "\n",
    "| CNN Architecture                             \t|\n",
    "|----------------------------------------------\t|\n",
    "| Conv: $C_{in}=1, C_{out}=32, K=3, S=1, P=0$  \t|\n",
    "| ReLU                                         \t|\n",
    "| Conv: $C_{in}=32, C_{out}=64, K=3, S=1, P=0$ \t|\n",
    "| ReLU                                         \t|\n",
    "| MaxPool2d: $K=2, S=2, P=0$                   \t|\n",
    "| Dropout: $p=0.25$                            \t|\n",
    "| Linear: $C_{in}=9216, C_{out}=128$           \t|\n",
    "| ReLU                                         \t|\n",
    "| Dropout: $p=0.5$                             \t|\n",
    "| Linear: $C_{in}=128, C_{out}=10$             \t|\n",
    "\n",
    "The layers needed: \n",
    "\n",
    "`nn.Conv2d,  nn.Linear,  nn.Dropout, nn.MaxPool2d, nn.Flatten`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Dropout(p=0.25),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(in_features=9216, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(in_features=128, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the defined sequential model\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if the forward pass works\n",
    "\n",
    "t = torch.randn(1,1,28,28)\n",
    "cnn = CNN()\n",
    "cnn(t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(cnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.293918\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.152264\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.131376\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.122954\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.046988\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.047992\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.141084\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.087032\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.115174\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.079967\n",
      "\n",
      "Train set: Average loss: 0.1136, Accuracy: 56711/60000 (94.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0513, Accuracy: 9833/10000 (98.330%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.073360\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.117034\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.249993\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.067190\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.126971\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.081015\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.026848\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.086379\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.137726\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.055024\n",
      "\n",
      "Train set: Average loss: 0.0991, Accuracy: 58649/60000 (97.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0385, Accuracy: 9883/10000 (98.830%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.089389\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.056941\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005934\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.021856\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.103037\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.039556\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.083302\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.026231\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.263714\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.113885\n",
      "\n",
      "Train set: Average loss: 0.0097, Accuracy: 58915/60000 (98.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 9891/10000 (98.910%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.037398\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.018870\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.023117\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.006772\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.006662\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.034753\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.129352\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.017000\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.005906\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.012643\n",
      "\n",
      "Train set: Average loss: 0.0055, Accuracy: 59073/60000 (98.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0334, Accuracy: 9885/10000 (98.850%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.004731\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.002312\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.039524\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.026175\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.018623\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.028634\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.012770\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.027188\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.029582\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.045191\n",
      "\n",
      "Train set: Average loss: 0.0249, Accuracy: 59242/60000 (98.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0348, Accuracy: 9901/10000 (99.010%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(cnn, dl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**99%** accuracy !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on CIFAR10\n",
    "\n",
    "Now we are going to train on CIFAR10. We can reuse most of the code above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "ds_train = datasets.CIFAR10(root='./data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is not normalized yet, so we need to calculate the normalization constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims_train = torch.tensor(ds_train.data)\n",
    "ims_train = ims_train.float() / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2470, 0.2435, 0.2616])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ims_train.std((0,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu = torch.mean(ims_train, dim=(0,1,2))\n",
    "std = torch.std(ims_train, dim=(0,1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4914, 0.4822, 0.4465])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(ims_train, dim=(0,1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CIFAR we want to make use of data augmentation to improve generalization.\n",
    "https://pytorch.org/docs/stable/torchvision/transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4 \n",
    "\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([transforms.RandomCrop(size=32, padding=4),transforms.RandomHorizontalFlip(p=0.4),transforms.ToTensor(), transforms.Normalize(mu,std)])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mu,std)])\n",
    "\n",
    "ds_train = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "ds_test = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the  optimizer, this time we use SGD. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 3, 3], expected input[128, 3, 32, 32] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     test(cnn, dl_test)    \n\u001b[1;32m      5\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dl_train, optimizer, epoch, log_interval, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()         \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m##### implement this part #####\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m output\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output,target)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/dl4cv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl4cv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[20], line 31\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Forward pass through the defined sequential model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl4cv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl4cv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/dl4cv/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/dl4cv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl4cv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/dl4cv/lib/python3.9/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl4cv/lib/python3.9/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 3, 3], expected input[128, 3, 32, 32] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(cnn, dl_test)    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Dropout(p=0.25),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(in_features=12544, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            \n",
    "            nn.Linear(in_features=128, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the defined sequential model\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN()(torch.randn(1,3,32,32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.303667\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.001911\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.833282\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.923108\n",
      "\n",
      "Train set: Average loss: 1.8310, Accuracy: 12679/50000 (25.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.6947, Accuracy: 3648/10000 (36.480%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.787641\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.746006\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.887616\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.759611\n",
      "\n",
      "Train set: Average loss: 1.7312, Accuracy: 15391/50000 (30.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.5848, Accuracy: 4067/10000 (40.670%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.717531\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.763910\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.655622\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.747837\n",
      "\n",
      "Train set: Average loss: 1.5089, Accuracy: 17601/50000 (35.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.5107, Accuracy: 4450/10000 (44.500%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.618753\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.732383\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.601515\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.740732\n",
      "\n",
      "Train set: Average loss: 1.7183, Accuracy: 19008/50000 (38.0%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.4683, Accuracy: 4784/10000 (47.840%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.667533\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.799055\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.757585\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.544064\n",
      "\n",
      "Train set: Average loss: 1.7048, Accuracy: 19035/50000 (38.1%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.5259, Accuracy: 4496/10000 (44.960%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.660695\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 1.786442\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 1.624403\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 1.840966\n",
      "\n",
      "Train set: Average loss: 1.5816, Accuracy: 19251/50000 (38.5%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.4810, Accuracy: 4685/10000 (46.850%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.815147\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 1.631607\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 1.793987\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 1.603719\n",
      "\n",
      "Train set: Average loss: 1.5214, Accuracy: 19349/50000 (38.7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.3883, Accuracy: 5062/10000 (50.620%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.598554\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 1.560735\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 1.765093\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 1.467105\n",
      "\n",
      "Train set: Average loss: 1.6839, Accuracy: 19892/50000 (39.8%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.4089, Accuracy: 4894/10000 (48.940%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.673212\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 1.542561\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 1.579009\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 1.586993\n",
      "\n",
      "Train set: Average loss: 1.3613, Accuracy: 20087/50000 (40.2%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.4346, Accuracy: 4767/10000 (47.670%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.546359\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 1.714214\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 1.744762\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 1.608264\n",
      "\n",
      "Train set: Average loss: 1.6041, Accuracy: 20178/50000 (40.4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.4553, Accuracy: 4601/10000 (46.010%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(cnn, dl_train, optimizer, epoch, log_interval=100)\n",
    "    test(cnn, dl_test)    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives 40 - 50 %. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "d268b61a0efacafa8645774cb6d0204c9f01d7563ef03f7672146d044e8f345c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
